{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with support vector machines\n",
    "\n",
    "We consider performing classification of MNIST digits using SVM, first for binary classification, then for multiclass classification. The reference page on scikit-learn for SVM is https://scikit-learn.org/stable/modules/svm.html (which you are encouraged to read!).\n",
    "\n",
    "**There are 10 questions to answer. Questions 5 to 8 can be skipped if the other questions are too time consuming.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up the random number generator: given seed for reproducibility, None otherwise\n",
    "# (see https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng)\n",
    "my_seed = 1\n",
    "rng = np.random.default_rng(seed=my_seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of the dataset \n",
    "\n",
    "We use the MNIST dataset already encountered in various hands-on sessions (kNN and PCA for instance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from Keras, values between 0 and 255 initially\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print('initial data type for images = ',x_train.dtype,', initial data shape = ',x_train.shape)\n",
    "print('initial data type for labels = ',y_train.dtype,', initial label shape = ',y_train.shape,'\\n')\n",
    "    \n",
    "# renormalize to have data between 0 and 1; could alternatively use built-in rescaling function\n",
    "# such as https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "x_train = x_train/255. \n",
    "x_test = x_test/255.\n",
    "print('Train set: data set size =',x_train.shape[0])\n",
    "print('Test set:  data set size =',x_test.shape[0])\n",
    "\n",
    "# reshape the data points, which are 28x28 tensors, into a single vector of size 28x28=784\n",
    "x_train = x_train.reshape((x_train.shape[0], 784))\n",
    "x_test = x_test.reshape((x_test.shape[0], 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the first elements of the resulting data set in order to see what they looks like, in particular when binarization is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    # color map = binary, other choices here https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    plt.imshow(x_train[i].reshape(28,28), cmap=plt.cm.binary)     \n",
    "    plt.title(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Classifying datasets of two digits only\n",
    "\n",
    "We start by a binary classification problem, for a dataset obtained by extracting images corresponding to two given digit values. The suggested digits are 0 and 8 since they can be visually quite similar.\n",
    "\n",
    "### Creating a dataset of two digits only\n",
    "\n",
    "We use the same routine as the one used for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_1 = 8\n",
    "digit_2 = 0\n",
    "\n",
    "# for train data\n",
    "indices_two_digits_train = np.where((y_train == digit_1)|(y_train == digit_2))\n",
    "x_train_two_digits, y_train_two_digits = x_train[indices_two_digits_train], y_train[indices_two_digits_train]\n",
    "print('Shape of train data: ',x_train_two_digits.shape)\n",
    "print('Shape of train labels:',y_train_two_digits.shape)\n",
    "\n",
    "# same for test\n",
    "indices_two_digits_test = np.where((y_test == digit_1)|(y_test == digit_2))\n",
    "x_test_two_digits, y_test_two_digits = x_test[indices_two_digits_test], y_test[indices_two_digits_test]\n",
    "print('Shape of test data: ',x_test_two_digits.shape)\n",
    "print('Shape of test labels:',y_test_two_digits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SVM on this database\n",
    "\n",
    "The parameter C is in factor of the penalty term arising from misclassification. A large value of C aims at classifying all training examples correctly. Various kernels can be used: \"linear\" (default choice), \"rbf\" (Gaussians), \"poly\" (polynomials), \"sigmoid\". Custom kernels could also be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by using SVM with some parameters set to default values.\n",
    "\n",
    "**Question 1.** What are the default values for $C$ and for the kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html on the function *svm.SVC()*, it appears that $C=1.0$ and kernel='rbf' by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier = svm.SVC()\n",
    "C_values = np.logspace(-1, 4, 6)\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for C_chosen in C_values:\n",
    "    # construct the classifier with given value of C and fit it\n",
    "    binary_classifier = svm.SVC(C=C_chosen)\n",
    "    binary_classifier.fit(x_train_two_digits,y_train_two_digits)\n",
    "    # compute the training error for the 0-1 loss\n",
    "    y_pred_two_digits = binary_classifier.predict(x_train_two_digits)\n",
    "    train_score = accuracy_score(y_true=y_train_two_digits, y_pred=y_pred_two_digits)\n",
    "    train_accuracy.append(train_score)\n",
    "    # compute the validation error for the 0-1 loss\n",
    "    y_pred_two_digits = binary_classifier.predict(x_test[indices_two_digits_test])\n",
    "    y_true_two_digits = y_test[indices_two_digits_test]\n",
    "    test_score = accuracy_score(y_true=y_true_two_digits, y_pred=y_pred_two_digits)\n",
    "    test_accuracy.append(test_score)\n",
    "    print('C =',C_chosen,': test accuracy =',test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the test accuracy as a function of the value of $C$ (at this stage we are not choosing the value of $C$, as this would require performing some cross validation with the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogx(C_values, train_accuracy,'-gD' ,color='red' , label=\"Train accuracy\")\n",
    "plt.semilogx(C_values, test_accuracy,'-gD' , label=\"Test Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Cost parameter C\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title('Accuracy as a function of cost parameter C (log-scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding and visualizing what SVM does\n",
    "\n",
    "We also have a closer look at the outputs for a given value of $C$, in particular the various coefficients, in order to understand the decision function. We consider the case of a linear kernel, which leads to more interpretable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier = svm.SVC(C=1,kernel=\"linear\")\n",
    "binary_classifier.fit(x_train_two_digits,y_train_two_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Plot the confusion matrix and report the accuracy on the test set for the parameters above. What is the fraction of *digit_1* classified as *digit_2*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# computing the accuracy\n",
    "indices_two_digits_test = np.where((y_test == digit_1)|(y_test == digit_2))\n",
    "y_pred_two_digits = binary_classifier.predict(x_test[indices_two_digits_test])\n",
    "y_true_two_digits = y_test[indices_two_digits_test]\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_true=y_true_two_digits, y_pred=y_pred_two_digits), \"\\n\")\n",
    "\n",
    "# computing the confusion matrix\n",
    "# see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "conf_mat = metrics.confusion_matrix(y_true=y_true_two_digits, y_pred=y_pred_two_digits)\n",
    "print(\"Confusion matrix\\n\",conf_mat,\"\\n\")\n",
    "\n",
    "# fraction of digit_1 misclassified as digit_2\n",
    "fraction = conf_mat[0,1]/(conf_mat[0,0]+conf_mat[0,1])\n",
    "print(\"Fraction of\",digit_1,\"misclassified as\",digit_2,\":\",fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next plot the weights in the shape of an image, to better visualize which pixels count more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = binary_classifier.coef_\n",
    "b = binary_classifier.intercept_\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.imshow(weights.reshape(28,28), cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next have a closer look at the values of the decision function.\n",
    "\n",
    "**Question 3.** Complete the code below to plot the histograms of the values of the decision function for each subcategory of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_pred_test = binary_classifier.decision_function(x_test)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "# we first plot the values of the decision function for all data points\n",
    "# can check that decision_pred_test[i] = np.dot(weights,x_test[i])+b\n",
    "plt.hist(decision_pred_test,bins=100,color='black',density=True)\n",
    "plt.xlabel(\"Value of the decision function\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title('Distribution of decision values')\n",
    "# we next look at the values for the images corresponding to digit_1 and digit_2, respectively\n",
    "indices_digit_1 = np.where((y_test == digit_1))\n",
    "decision_pred_test_1 = binary_classifier.decision_function(x_test[indices_digit_1])\n",
    "indices_digit_2 = np.where((y_test == digit_2))\n",
    "decision_pred_test_2 = binary_classifier.decision_function(x_test[indices_digit_2])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(decision_pred_test_1,bins=50,color='royalblue',density=True)\n",
    "plt.hist(decision_pred_test_2,bins=50,color='red',density=True)\n",
    "plt.xlabel(\"Value of the decision function\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend(['digit_1','digit_2'])\n",
    "plt.title('Distribution of decision values for each digit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next investigate the support vectors.\n",
    "\n",
    "**Question 4.** Complete the code below to identify the support vector and plot some of them. Use the attributes *support_vectors_* of the SVC class https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of support vectors for each class:')\n",
    "print(' - for',digit_1,': ',binary_classifier.n_support_[0])\n",
    "print(' - for',digit_2,': ',binary_classifier.n_support_[1],'\\n')\n",
    "indices = binary_classifier.support_\n",
    "print('Some support vectors')\n",
    "decision_values = binary_classifier.decision_function(binary_classifier.support_vectors_)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(5):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(binary_classifier.support_vectors_[i].reshape(28,28), cmap=plt.cm.binary)\n",
    "    plt.title(round(decision_values[i],5))\n",
    "    plt.subplot(5, 5, i+6)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(binary_classifier.support_vectors_[-i-1].reshape(28,28), cmap=plt.cm.binary)\n",
    "    plt.title(round(decision_values[-i-1],5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing SVM after a PCA\n",
    "\n",
    "We next perform a PCA, to reduce the dimensionality of the inputs to two variables, and then perform SVM in this reduced space. We do this for pedagogical reasons, to allow for a better visualization of the results, as this degrades the accuracy of the prediction. One can however increase the number of retained PCA dimensions until the accuracy on the predictions is nearly as good as with the full features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA()\n",
    "pca.n_components = 784\n",
    "pca_data = pca.fit_transform(x_train_two_digits)\n",
    "percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_)\n",
    "cum_var_explained = np.cumsum(percentage_var_explained)\n",
    "# Plot the PCA spectrum\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4))\n",
    "ax1.plot(pca.explained_variance_, linewidth=2)\n",
    "ax1.grid()\n",
    "ax1.set_xlabel('Index of component')\n",
    "ax1.set_ylabel('Covariance eigenvalue')\n",
    "ax2.plot(cum_var_explained, linewidth=2)\n",
    "ax2.grid()\n",
    "ax2.set_xlabel('Number of components')\n",
    "ax2.set_ylabel('Fraction of cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (not priority).** Compute the fraction of variance explained by the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraction of explained variance for dimension 2:',cum_var_explained[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform images of both digits into 2 dimensional vectors for plotting, by keeping some PCA components. \n",
    "\n",
    "**Question 6 (not priority).** Compute the scores of the test images on the first two principal components, and perform the scatter plot of these scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of PCA components to plot\n",
    "pca.n_components = 2\n",
    "component_1 = 0\n",
    "component_2 = 1\n",
    "\n",
    "pca.fit(x_train_two_digits)\n",
    "score_train = pca.transform(x_train_two_digits)\n",
    "score_test = pca.transform(x_test_two_digits)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(score_test[:,component_1],score_test[:,component_2],s=0.3,color='black')\n",
    "plt.subplot(1, 2, 2)\n",
    "score_test_digit_1 = pca.transform(x_test[indices_digit_1])\n",
    "score_test_digit_2 = pca.transform(x_test[indices_digit_2])\n",
    "plt.scatter(score_test_digit_1[:,component_1],score_test_digit_1[:,component_2],s=0.3,color='royalblue')\n",
    "plt.scatter(score_test_digit_2[:,component_1],score_test_digit_2[:,component_2],s=0.3,color='red')\n",
    "plt.legend(['digit_1','digit_2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems almost possible to linearly separate the data points as such, so we try SVM. One can test the quality of the results depending on the chosen kernel.\n",
    "\n",
    "**Question 7 (not priority).** Which kernel gives the best performance for $C=10$ after reducing to two principal components? How does the performance compare to the situation where no PCA is performed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classifier_PCA = svm.SVC(C=10,kernel=\"linear\")\n",
    "binary_classifier_PCA.fit(score_train,y_train_two_digits)\n",
    "y_train_PCA = binary_classifier_PCA.predict(score_train)\n",
    "y_test_PCA = binary_classifier_PCA.predict(score_test)\n",
    "print(\"accuracy (on train data):\", metrics.accuracy_score(y_true=y_train_two_digits,y_pred=y_train_PCA))\n",
    "print(\"accuracy (on test data) :\", metrics.accuracy_score(y_true=y_test_two_digits,y_pred=y_test_PCA), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rbf and linear give very similar results, with an accuracy of about 95%, poly is slightly less good, sigmoid is significantly worse. When no PCA is performed, the test accuracy is much better, above 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8 (not priority).** For the classifier with the linear kernel, plot the level sets of the decision function for the values -1,0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- plot first the scatter plot of the data set (re-use what has been done above) --\n",
    "plt.figure(figsize=(8, 5))\n",
    "score_test_digit_1 = pca.transform(x_test[indices_digit_1])\n",
    "score_test_digit_2 = pca.transform(x_test[indices_digit_2])\n",
    "plt.scatter(score_test_digit_1[:,component_1],score_test_digit_1[:,component_2],s=0.5,color='royalblue')\n",
    "plt.scatter(score_test_digit_2[:,component_1],score_test_digit_2[:,component_2],s=0.5,color='red')\n",
    "plt.legend(['digit_1','digit_2'])\n",
    "#-- plot next the required isolines --\n",
    "w = binary_classifier_PCA.coef_[0]\n",
    "b = binary_classifier_PCA.intercept_\n",
    "# isoline 0 of the decision function: equation w1*x1 + w2*x2 + b = 0, so x2 = -(w1*x1+b)/w2\n",
    "x1 = np.linspace(-6,7,20)\n",
    "x2 = -(w[0]*x1+b)/w[1]\n",
    "plt.plot(x1,x2,color='black')\n",
    "# isoline -1 of the decision function: equation w1*x1 + w2*x2 + b = -1, so x2 = -(w1*x1+b)/w2\n",
    "x2 = -(w[0]*x1+b+1)/w[1]\n",
    "plt.plot(x1,x2,color='black',linestyle='dashed')\n",
    "# isoline 1 of the decision function: equation w1*x1 + w2*x2 + b = 1, so x2 = -(w1*x1+b)/w2\n",
    "x2 = -(w[0]*x1+b-1)/w[1]\n",
    "plt.plot(x1,x2,color='black',linestyle='dashed')\n",
    "plt.ylim((-6,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Multiclass SVM\n",
    "\n",
    "We now turn to classification for all digits; see Section 1.4.1.1 in https://scikit-learn.org/stable/modules/svm.html\n",
    "The training is now more intensive, and can take several minutes -- which is why we subsample the dataset in order to allow for a faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampling_rate = 10\n",
    "# start:sto:step \n",
    "x_train_subsampled = x_train[0::10]\n",
    "y_train_subsampled = y_train[0::10]\n",
    "print('Initial dimension of the data:',x_train.shape)\n",
    "print('After subsampling at rate',subsampling_rate,':',x_train_subsampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_classifier = svm.SVC(C=10,kernel=\"linear\")\n",
    "multi_classifier.fit(x_train_subsampled,y_train_subsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9.** Which strategy is used for classification: one-vs-all or one-vs-one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC it appears that the method uses the one-vs-one paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = multi_classifier.predict(x_test)\n",
    "y_true = y_test\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true, y_pred), \"\\n\")\n",
    "print(metrics.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at misclassified digits.\n",
    "\n",
    "**Question 9.** Complete the code below to find misclassified digits to plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_digit = 6\n",
    "prediction_digit = 4\n",
    "misclassification_indices = np.where((y_test == true_digit)&(y_pred == prediction_digit))\n",
    "misclassified_images = x_test[misclassification_indices]\n",
    "if (misclassified_images.shape[0] == 0):\n",
    "    print('No',true_digit,'classified as',prediction_digit)\n",
    "else:\n",
    "    print('Example of',true_digit,'classified as',prediction_digit)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(min(5,misclassified_images.shape[0])):\n",
    "       plt.subplot(5, 5, i+1)\n",
    "       plt.xticks([])\n",
    "       plt.yticks([])\n",
    "       plt.grid(False)\n",
    "       plt.imshow(misclassified_images[i].reshape(28,28), cmap=plt.cm.binary)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10.** Test various kernels and values of $C$. What is the best prediction error you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_classifier = svm.SVC(C=1,kernel=\"rbf\")\n",
    "multi_classifier.fit(x_train_subsampled,y_train_subsampled)\n",
    "y_pred = multi_classifier.predict(x_test)\n",
    "y_true = y_test\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true, y_pred), \"\\n\")\n",
    "print(metrics.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian kernels give better results than linear ones. With $C=10$ and 'rbf' as kernel, the accuracy is 0.959; while 'linear' gives 0.90, 'poly' 0.942 and 'sigmoid' 0.755. For 'rbf', $C=1$ leads to 0.950, and 0.959 for $C=100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Extensions for final project\n",
    "\n",
    "Here is a list of possible extensions:\n",
    "- implement cross validation using *GridSearchCV* (see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html);\n",
    "- plot learning curves (test and train losses as a function of the sample size);\n",
    "- recognizing your own written digits: see https://towardsdatascience.com/support-vector-machine-mnist-digit-classification-with-python-including-my-hand-written-digits-83d6eca7004a to write your own numbers and have them classified! In essence, produce an image with some software, and then resize it using (under Linux) *convert -resize 28X28! sample_image0.png sample_image0_r.png* This should produce a quite small image at a low resolution. Start by checking the image and converting it to a 28 x 28 numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
