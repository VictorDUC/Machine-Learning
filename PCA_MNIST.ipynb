{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis\n",
    "\n",
    "We use here PCA to perform dimensionality reduction, first to explore how the method works and how well its perform in compressing the data at hand; and then whether it is a useful preprocessing of other learning algorithms (here, logistic regression).\n",
    "\n",
    "**There are 5 questions to answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up the random number generator: given seed for reproducibility, None otherwise\n",
    "# (see https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng)\n",
    "my_seed = 1\n",
    "rng = np.random.default_rng(seed=my_seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n",
    "we consider performing PCA on the standard MNIST dataset already encountered in few times in the hands-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from Keras, values between 0 and 255 initially\n",
    "(x_train_full, y_train_full), (x_test_full, y_test_full) = tf.keras.datasets.mnist.load_data()\n",
    "print('initial data type for images = ',x_train_full.dtype,', initial data shape = ',x_train_full.shape)\n",
    "print('initial data type for labels = ',y_train_full.dtype,', initial label shape = ',y_train_full.shape,'\\n')\n",
    "    \n",
    "# renormalize to have data between 0 and 1; could alternatively use built-in rescaling function\n",
    "# such as https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "x_train_full = x_train_full/255. \n",
    "x_test_full = x_test_full/255.\n",
    "print('Train set: data set size =',x_train_full.shape[0])\n",
    "print('Test set:  data set size =',x_test_full.shape[0])\n",
    "\n",
    "# reshape the data points, which are 28x28 tensors, into a single vector of size 28x28=784\n",
    "x_train_full = x_train_full.reshape((x_train_full.shape[0], 784))\n",
    "x_test_full = x_test_full.reshape((x_test_full.shape[0], 784))\n",
    "\n",
    "# shuffle data\n",
    "indices = np.random.permutation(x_train_full.shape[0])\n",
    "x_train_full = x_train_full[indices]\n",
    "y_train_full = y_train_full[indices]\n",
    "indices = np.random.permutation(x_test_full.shape[0])\n",
    "x_test_full = x_test_full[indices]\n",
    "y_test_full = y_test_full[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the first elements of the resulting data set in order to see what they looks like, in particular when binarization is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    # color map = binary, other choices here https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    plt.imshow(x_train_full[i].reshape(28,28), cmap=plt.cm.binary)     \n",
    "    plt.title(y_train_full[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing PCA\n",
    "\n",
    "We use the PCA as implemented in scikit-learn https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html, see also https://scikit-learn.org/stable/modules/decomposition.html#pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required class\n",
    "from sklearn import decomposition\n",
    "\n",
    "# initializing the pca\n",
    "pca = decomposition.PCA()\n",
    "pca.n_components = 784\n",
    "pca_data = pca.fit_transform(x_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the various eigenvalues of the empirical covariance, and the percentage of the explained variance as a function of the eigenvalue number.\n",
    "\n",
    "**Question 1.** Compute the percentage of explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_)\n",
    "cum_var_explained = np.cumsum(percentage_var_explained)\n",
    "# alternatively, one could directly use \n",
    "# cum_var_explained = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA spectrum\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4))\n",
    "ax1.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "#ax1.semilogy(pca.explained_variance_, linewidth=2)\n",
    "ax1.grid()\n",
    "ax1.set_xlabel('Index of component')\n",
    "ax1.set_ylabel('Covariance eigenvalue')\n",
    "ax1.set_xlim(0,100)\n",
    "ax2.plot(cum_var_explained, linewidth=2)\n",
    "ax2.grid()\n",
    "ax2.set_xlabel('Number of components')\n",
    "ax2.set_ylabel('Fraction of cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Write a function which gives the number of components required to attain a certain percentage of the total variance. How many components are needed to retrieve 50% of the total variance? 80%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.5\n",
    "booleans_cum_var = (cum_var_explained < fraction).astype('uint8')\n",
    "index = booleans_cum_var.argmin()+1\n",
    "print(\"For a fraction\",fraction,\"of the total variance, one needs\",index,\"components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next study what images look like when dimensionality reduction is performed.\n",
    "\n",
    "**Question 3.** Complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try PCA on first images\n",
    "n_images = 8 \n",
    "vector_of_index_values = [700,200,120,80,30,10]\n",
    "\n",
    "n_index_values = len(vector_of_index_values)\n",
    "plt.figure(figsize=(n_images*2, n_index_values*2))\n",
    "\n",
    "current_index_value = 0\n",
    "for i in range(n_images):\n",
    "    plt.subplot(n_index_values+1, n_images, current_index_value*n_images+i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    if i == 0:\n",
    "        plt.ylabel('reference')\n",
    "    plt.imshow(x_test_full[i].reshape(28,28), cmap=plt.cm.binary)     \n",
    "\n",
    "for current_index_value in range(1,n_index_values+1):\n",
    "  pca.n_components = vector_of_index_values[current_index_value-1]\n",
    "  pca.fit(x_train_full)\n",
    "  # pca.transform gives the score along the principal directions\n",
    "  # pca.inverse_transform maps back to the original space\n",
    "  outputs = pca.inverse_transform(pca.transform(x_test_full))\n",
    "  for i in range(n_images):\n",
    "      plt.subplot(n_index_values+1, n_images, current_index_value*n_images+i+1)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.grid(False)\n",
    "      if i == 0:\n",
    "          plt.ylabel('n_comp ='+str(pca.n_components))\n",
    "      plt.imshow(outputs[i].reshape(28,28), cmap=plt.cm.binary)     \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** In which range of values are the pixels for the reconstructed images when 50 components are retained? What do you think of this result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components = 50\n",
    "pca.fit(x_train_full)\n",
    "outputs = pca.inverse_transform(pca.transform(x_test_full))\n",
    "print(\"The minimal pixel value over all test images is\",outputs.min(),\"while the maximum is\",outputs.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no mechanism in PCA ensuring that the values of the reconstructed images remain between 0 and 1. However, as the number of components is increased, the minimum of the pixels increases to 0, while the maximum decreases to 1 (somewhat slowly however... it would be better probably to plot the distribution of minimum/maximum for each image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Complete the code below to plot the projections of the test images onto the first factorial plane, by coloring each point according to its label. Can you separate the images in this representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA, the first 2 components are sufficient\n",
    "pca.n_components = 2\n",
    "pca.fit(x_train_full)\n",
    "test_scores = pca.transform(x_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = np.arange(0,10)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(test_scores[:,0],test_scores[:,1],s=0.1,color='black')\n",
    "plt.title('all digits')\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in digits:\n",
    "    indices = np.where((y_test_full == i))\n",
    "    test_scores_extracted = test_scores[indices]\n",
    "    plt.scatter(test_scores_extracted[:,0],test_scores_extracted[:,1],s=0.5)\n",
    "    plt.legend(digits)\n",
    "    #plt.title('subset of the digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images cannot be separated in this representation. It is even difficult to form groups. Dimension has been too drastically reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional topics for project\n",
    "\n",
    "One can revisit logistic regression after performing dimensionality reduction, and determine how the classification performance and the computational cost in training are impacted by the dimensionality reduction.\n",
    "One can also check the impact of various alternative normalizations of the data (such as MinMaxScaler(), Standardize(), or other options described in the page https://scikit-learn.org/stable/modules/preprocessing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
